{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Part 1. Data loading and environment setting"
      ],
      "metadata": {
        "id": "aR0My8sfgOZZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5aTxynee2HU"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install bertopic\n",
        "#!pip install cohere\n",
        "!pip install altair"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6i15m_BPfKk-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import seaborn as sns\n",
        "import requests\n",
        "import re\n",
        "from tabulate import tabulate\n",
        "from matplotlib.pyplot import figure\n",
        "import seaborn.objects as so\n",
        "import scipy.stats\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "from bertopic import BERTopic\n",
        "from bertopic.vectorizers import ClassTfidfTransformer\n",
        "#import cohere\n",
        "from bertopic.representation import Cohere\n",
        "from bertopic.backend import CohereBackend\n",
        "import umap\n",
        "import altair as alt\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from scipy.cluster import hierarchy\n",
        "\n",
        "import bigframes.pandas as bpd\n",
        "import tensorflow_hub\n",
        "from bertopic.representation import KeyBERTInspired\n",
        "from transformers.pipelines import pipeline\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import colorcet as cc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3DYSeu76uLf6"
      },
      "outputs": [],
      "source": [
        "# additional\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.colors as mcolors\n",
        "\n",
        "\n",
        "# stopwords\n",
        "import nltk\n",
        "nltk.download(['wordnet', 'stopwords', 'punkt'])\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# load NLTK's list of English stopwords\n",
        "stop_words = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QfrcRgUXuLJM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8A-6b9GNuET"
      },
      "source": [
        "### Part 2. Loading message data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k254vQ1k1vh2"
      },
      "outputs": [],
      "source": [
        "!pip install nltk\n",
        "!pip install wordcloud matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6Fad5SSa_8U"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "def download_nltk_resources():\n",
        "    \"\"\"\n",
        "    Ensure that necessary NLTK resources are available.\n",
        "    Download them only if they are not already downloaded.\n",
        "    \"\"\"\n",
        "    resources = {\n",
        "        'tokenizers/punkt': 'punkt',\n",
        "        'corpora/wordnet': 'wordnet',\n",
        "        'taggers/averaged_perceptron_tagger': 'averaged_perceptron_tagger'\n",
        "    }\n",
        "\n",
        "    for path, package in resources.items():\n",
        "        try:\n",
        "            nltk.data.find(path)\n",
        "        except LookupError:\n",
        "            nltk.download(package)\n",
        "\n",
        "# Call the function to check and download resources\n",
        "download_nltk_resources()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing special characters is optional. We did this because some messages were saved in a rft format."
      ],
      "metadata": {
        "id": "mnVrgVlrd4ub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WPp8l3IBbFxV"
      },
      "outputs": [],
      "source": [
        "def remove_special_patterns(text):\n",
        "    # This function can be adjusted if specific unwanted patterns are observed\n",
        "    text = re.sub(r'\\{[^ ]* ', ' ', text)\n",
        "    text = re.sub(r'\\\\[^ ]* ', ' ', text)\n",
        "    text = re.sub(r'Arial;}}[^ ]* ', ' ', text)\n",
        "    text = re.sub(r';[^ ]* ', ' ', text)\n",
        "    text = re.sub(r',\\\\[^ ]* ', ' ', text)\n",
        "    return text\n",
        "\n",
        "def clean_and_remove_rtf(text):\n",
        "    # Remove RTF control words, formatting codes, and unnecessary curly braces content\n",
        "    text = re.sub(r'\\\\[a-zA-Z]+\\d* ?', '', text)  # Removes control words with optional numbers\n",
        "    text = re.sub(r'\\\\[a-zA-Z]+', '', text)  # Cleans any remaining control words\n",
        "    text = re.sub(r'{[^{}]*}', '', text)  # Aggressively remove content within curly braces\n",
        "    text = re.sub(r'\\btsWidth\\d*\\b', '', text)  # Specific removal of 'tsWidth' followed by any numbers\n",
        "    text = re.sub(r'\\bcl[a-zA-Z0-9]+\\b', '', text)  # Removes words starting with 'cl' that are part of cell definitions\n",
        "    text = re.sub(r'row[a-zA-Z0-9]+\\b', '', text)  # Remove patterns starting with 'row' typical in table definitions\n",
        "    text = re.sub(r'\\brd[a-zA-Z0-9]+', '', text)  # Remove 'rd' prefixed RTF controls like 'rdrnone'\n",
        "    text = re.sub(r'\\b[a-zA-Z0-9]{1,3}\\b', '', text)  # Remove isolated short words\n",
        "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)  # Remove non-ASCII characters\n",
        "    text = re.sub(r'\\s{2,}', ' ', text).strip()  # Normalize whitespace\n",
        "    text = re.sub(r'\\bArial\\b', '', text, flags=re.IGNORECASE)  # Remove the name 'Arial' completely\n",
        "    return text\n",
        "\n",
        "\n",
        "def get_wordnet_pos(tag):\n",
        "    \"\"\"Map NLTK POS tag to a format recognized by WordNetLemmatizer\"\"\"\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "    return tag_dict.get(tag[0].upper(), wordnet.NOUN)\n",
        "\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = word_tokenize(text)\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in tokens]\n",
        "    return ' '.join(lemmatized_tokens)\n",
        "\n",
        "\n",
        "def process_msg_txt(msg):\n",
        "    \"\"\"\n",
        "    Process message texts such that they are standardized.\n",
        "    \"\"\"\n",
        "    if isinstance(msg, str):\n",
        "        msg = remove_special_patterns(msg)\n",
        "        msg = clean_and_remove_rtf(msg)\n",
        "        return lemmatize_text(msg) if msg else \"\"\n",
        "    return \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91OPZ_Q84BzA"
      },
      "outputs": [],
      "source": [
        "def filter_msg_txt(row):\n",
        "    \"\"\"\n",
        "    Assign messages that meet criteria for a relevant message a value of 1,\n",
        "    otherwise assign value of 0.\n",
        "\n",
        "    Criteria:\n",
        "      - msg is > 50 characters\n",
        "      - subject isn't standard message for survey for recent video visit\n",
        "    \"\"\"\n",
        "    if len(row[\"msg_txt_processed\"]) <= 50:\n",
        "        return 0\n",
        "    elif row[\"msg_txt_processed\"].startswith(\"We were unable to reach you by phone.\"):\n",
        "        return 0\n",
        "    else:\n",
        "        return 1\n",
        "\n",
        "def process_filter_msg_df(msg_df):\n",
        "    \"\"\"\n",
        "    In: msg_df in the form of ABC_0000000000{0:0=2d}.csv\n",
        "    Out: msg_df that has been processed and filtered\n",
        "    \"\"\"\n",
        "    msg_df[\"msg_txt_processed\"] = msg_df[\"msg_txt\"].apply(process_msg_txt)\n",
        "    msg_df[\"msg_txt_flag\"] = msg_df.apply(filter_msg_txt, axis=1)\n",
        "    msg_processed_filtered_df = msg_df[msg_df[\"msg_txt_flag\"] == 1]\n",
        "    msg_processed_filtered_df = msg_processed_filtered_df.drop_duplicates(subset=[\"msg_txt_processed\"], keep=\"first\")\n",
        "    return msg_processed_filtered_df\n",
        "\n",
        "\n",
        "\n",
        "def filter_by_id(msg_df, id_arr):\n",
        "    \"\"\"\n",
        "    In: msg_df and array of IDs to filter by (i.e., ID arr of patients)\n",
        "    Out: msg_df with only relevant patients\n",
        "    \"\"\"\n",
        "    return msg_df.loc[msg_df[\"anon_id\"].isin(id_arr)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BgPkO7HlukVJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def get_diagnosis(row):\n",
        "  \"\"\"\n",
        "  Helper function to change ICD-10 codes into diagnoses.\n",
        "  \"\"\"\n",
        "\n",
        "  if row[\"icd10\"].startswith(\"C43\"):\n",
        "    return \"Melanoma\"\n",
        "  elif row[\"icd10\"].startswith(\"C44\"):\n",
        "    return \"Other malignant of skin\"\n",
        "  elif row[\"icd10\"].startswith(\"C46\"):\n",
        "    return \"Kaposi Sarcoma\"\n",
        "  else:\n",
        "    print(row[\"icd10\"])\n",
        "    raise Exception(\"Invalid ICD-10 code\")\n",
        "\n",
        "def get_diagnostic_grouping(row):\n",
        "  \"\"\"\n",
        "  Helper function to change diagnoses into diagnostic groupings.\n",
        "  \"\"\"\n",
        "\n",
        "  if row[\"diagnosis\"] in [\"Melanoma\", \"Other malignant of skin\", \"Kaposi Sarcoma\"]:\n",
        "    return \"Skin cancer\"\n",
        "\n",
        "  else:\n",
        "    print(row[\"diagnosis\"])\n",
        "    raise Exception(\"Invalid Diagnosis\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# retreiving anonimous_id of those with skin cancer ICD-10 codes\n",
        "\n",
        "from google.cloud import storage\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Configure your GCS bucket and file\n",
        "bucket_name = \"Your-bucket-name\"  # Replace with your GCS bucket name\n",
        "file_path = \"ABC.csv\"  # Replace with your file's path in the bucket\n",
        "local_file_path = \"path/ABC.csv\"  # Local file path to save the downloaded file\n",
        "\n",
        "# Ensure the local directory exists\n",
        "local_directory = os.path.dirname(local_file_path)\n",
        "os.makedirs(local_directory, exist_ok=True)\n",
        "\n",
        "# Download file from GCS\n",
        "try:\n",
        "    client = storage.Client()\n",
        "    bucket = client.get_bucket(bucket_name)\n",
        "    blob = bucket.blob(file_path)\n",
        "    blob.download_to_filename(local_file_path)  # Save locally\n",
        "    print(f\"File downloaded to {local_file_path}.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "\n",
        "# Load the dataset\n",
        "try:\n",
        "    com_id_df = pd.read_csv(local_file_path)\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Failed to load dataset: {e}\")"
      ],
      "metadata": {
        "id": "xrHmd360uG78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "471Fb1S6vaXg"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Convert ICD codes into diagnoses\n",
        "com_id_df[\"diagnosis\"] = com_id_df.apply(lambda row: get_diagnosis(row), axis=1)\n",
        "\n",
        "# Convert diagnoses into diagnostic groupings\n",
        "com_id_df[\"Diagnostic Group\"] = com_id_df.apply(lambda row: get_diagnostic_grouping(row), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZUtJAQ8vqZP"
      },
      "outputs": [],
      "source": [
        "# If patient ID with same diagnosis is listed multiple times,\n",
        "# only keep one entry.\n",
        "com_id_df = com_id_df.drop_duplicates(subset=[\"anon_id\", \"diagnosis\"], keep=\"first\")\n",
        "print(com_id_df.shape)\n",
        "\n",
        "# If patient ID exists for 2+ diagnoses, drop the patient ID.\n",
        "com_id_df = com_id_df.drop_duplicates(subset=[\"anon_id\"], keep=False)\n",
        "print(com_id_df.shape)\n",
        "\n",
        "# Get valid IDs\n",
        "final_valid_ids = com_id_df[\"anon_id\"].values\n",
        "print(final_valid_ids)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the message data from CGP bucket\n",
        "\n",
        "import pandas as pd\n",
        "from io import StringIO  # Import StringIO from io module\n",
        "from google.cloud import storage\n",
        "from google.api_core.exceptions import NotFound\n",
        "\n",
        "# Initialize Google Cloud Storage client\n",
        "client = storage.Client()\n",
        "bucket_name = 'Your-bucket-name'\n",
        "bucket = client.get_bucket(bucket_name)\n",
        "\n",
        "# Define a function to download CSV file and handle errors\n",
        "def download_blob(file_name):\n",
        "    try:\n",
        "        blob = bucket.blob(file_name)\n",
        "        content = blob.download_as_text()  # Download content as a string\n",
        "        return pd.read_csv(StringIO(content))  # Read into DataFrame\n",
        "    except NotFound:\n",
        "        print(f\"Error: The file '{file_name}' does not exist in the bucket '{bucket_name}'.\")\n",
        "        return pd.DataFrame()  # Return an empty DataFrame if the file is not found\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while downloading '{file_name}': {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "# Empty DataFrame for concatenating results\n",
        "msg_com_all_df = pd.DataFrame()\n",
        "\n",
        "# Run pipeline for all 15 subsets of the dataset\n",
        "for num in range(15):  # 0 to 14\n",
        "    # Construct the file name based on the loop index\n",
        "    file_name = f\"ABC0000000000{num:02}.csv\"\n",
        "\n",
        "    # Download the DataFrame from the bucket\n",
        "    temp_msg_all_df = download_blob(file_name)\n",
        "\n",
        "    if temp_msg_all_df.empty:\n",
        "        print(f\"No data to process for {file_name}. Skipping...\")\n",
        "        continue  # Skip to the next file if the DataFrame is empty\n",
        "\n",
        "    # Perform the processing on the DataFrame\n",
        "    temp_msg_all_df = process_filter_msg_df(temp_msg_all_df)\n",
        "    temp_msg_com_df = filter_by_id(temp_msg_all_df, final_valid_ids)\n",
        "\n",
        "    # Concatenate the processed DataFrame\n",
        "    msg_com_all_df = pd.concat([msg_com_all_df, temp_msg_com_df], ignore_index=True)\n",
        "\n",
        "# Print the shape of the DataFrame\n",
        "    print(f'Processed shape after loading {file_name}: {msg_com_all_df.shape}')\n",
        "\n",
        "# Final DataFrame is now in msg_com_all_df"
      ],
      "metadata": {
        "id": "am_ctgCsM-0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HlU2XnKbceRy"
      },
      "outputs": [],
      "source": [
        "# Add ICD-10 diagnosis and diagnostic group to the dataframe of messages\n",
        "msg_com_all_df = msg_com_all_df.merge(com_id_df[[\"anon_id\", \"diagnosis\", \"Diagnostic Group\"]], left_on=\"anon_id\", right_on=\"anon_id\")\n",
        "msg_com_all_df[\"diagnosis\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOgnIyHDzMZL"
      },
      "outputs": [],
      "source": [
        "print(msg_com_all_df.shape)\n",
        "print(msg_com_all_df.columns.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAq0l4DEzPoU"
      },
      "outputs": [],
      "source": [
        "#check the first 5 lines of data\n",
        "msg_com_all_df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-1yXvXJzc4h"
      },
      "outputs": [],
      "source": [
        "# Create column to flag columns that meet criteria\n",
        "msg_com_all_df[\"msg_txt_processed\"] = msg_com_all_df.apply(lambda row: process_msg_txt(row[\"msg_txt\"]), axis=1)\n",
        "msg_com_all_df[\"msg_txt_flag\"] = msg_com_all_df.apply(lambda row: filter_msg_txt(row), axis=1)\n",
        "\n",
        "# Create dataframe containing only messages that meet processed criteria\n",
        "msg_processed_df = msg_com_all_df[msg_com_all_df[\"msg_txt_flag\"] == 1]\n",
        "\n",
        "# Drop rows with duplicate messages\n",
        "msg_processed_df = msg_processed_df.drop_duplicates(subset=[\"msg_txt_processed\"], keep=\"first\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSzix1ZEzhi3"
      },
      "outputs": [],
      "source": [
        "msg_com_all_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4OgcATtTzlIF"
      },
      "outputs": [],
      "source": [
        "# Create list of messages for formatting into model\n",
        "short_docs = msg_processed_df[\"msg_txt_processed\"].values.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOhxXLSNzoeG"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Pre-calculate embeddings\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "embeddings = embedding_model.encode(short_docs, show_progress_bar=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save embeddings to a CSV file so we don't have to re-run everytime\n",
        "embeddings_df = pd.DataFrame(embeddings)\n",
        "embeddings_file_path = \"embeddings.csv\"\n",
        "embeddings_df.to_csv(embeddings_file_path, index=False)\n",
        "\n",
        "def upload_to_bucket(bucket_name, source_file_name, destination_blob_name):\n",
        "    \"\"\"Uploads a file to the bucket.\"\"\"\n",
        "\n",
        "    # Initialize a storage client\n",
        "    storage_client = storage.Client()\n",
        "\n",
        "    # Get the bucket\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "\n",
        "    # Create a blob object from the bucket\n",
        "    blob = bucket.blob(destination_blob_name)\n",
        "\n",
        "    # Upload the file\n",
        "    blob.upload_from_filename(source_file_name)\n",
        "\n",
        "    print(f\"File {source_file_name} uploaded to {destination_blob_name}.\")\n",
        "\n",
        "# Define your bucket name\n",
        "bucket_name = \"Your-bucket-name\"\n",
        "\n",
        "# File paths\n",
        "source_file_name = embeddings_file_path  # Use the path of the generated embeddings file\n",
        "destination_blob_name = \"path/emb_ABC.csv\"  # Change the path if needed\n",
        "\n",
        "# Upload the file\n",
        "upload_to_bucket(bucket_name, source_file_name, destination_blob_name)"
      ],
      "metadata": {
        "id": "QcOVGlwzNY5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-load the saved embeddings\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.cloud import storage\n",
        "\n",
        "def download_from_bucket(bucket_name, source_blob_name, destination_file_name):\n",
        "    \"\"\"Downloads a file from the bucket.\"\"\"\n",
        "\n",
        "    # Initialize a storage client\n",
        "    storage_client = storage.Client()\n",
        "\n",
        "    # Get the bucket\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "\n",
        "    # Create a blob object from the bucket\n",
        "    blob = bucket.blob(source_blob_name)\n",
        "\n",
        "    # Download the file\n",
        "    blob.download_to_filename(destination_file_name)\n",
        "\n",
        "    print(f\"File {source_blob_name} downloaded to {destination_file_name}.\")\n",
        "\n",
        "# Define your bucket name and file paths\n",
        "bucket_name = \"Your-bucket-name\"\n",
        "source_blob_name = \"path/emb_ABC.csv\"  # The path you uploaded to\n",
        "destination_file_name = \"loaded_embeddings.csv\"  # Local path where you want to save the downloaded file\n",
        "\n",
        "# Download the file from the bucket\n",
        "download_from_bucket(bucket_name, source_blob_name, destination_file_name)\n",
        "\n",
        "# Load embeddings from the downloaded CSV file\n",
        "loaded_embeddings_df = pd.read_csv(destination_file_name)\n",
        "\n",
        "# Convert DataFrame back to a NumPy array if needed\n",
        "loaded_embeddings = loaded_embeddings_df.to_numpy()\n",
        "\n",
        "# Verify the shape of the loaded embeddings\n",
        "print(f\"Loaded embeddings shape: {loaded_embeddings.shape}\")"
      ],
      "metadata": {
        "id": "maHfHLRRNeoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4onSI2A23yq"
      },
      "source": [
        "### Part 3. a. Topic model (1st BERTopic model)###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "US1mo-LV8lw6"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import Birch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from bertopic import BERTopic\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from bertopic.representation import KeyBERTInspired\n",
        "\n",
        "# Define your stop words and zero-shot topic list\n",
        "zeroshot_common_topic_list = [\"cancer\"]\n",
        "stop_words_list = ['Your-custom-stopwords-here']\n",
        "\n",
        "vectorizer_model = CountVectorizer(stop_words=stop_words_list)\n",
        "\n",
        "# Define your BERTopic model\n",
        "topic_model = BERTopic(\n",
        "    embedding_model=embedding_model,\n",
        "    vectorizer_model=vectorizer_model,\n",
        "    min_topic_size=15,\n",
        "    zeroshot_topic_list=zeroshot_common_topic_list,\n",
        "    zeroshot_min_similarity=.82,\n",
        "    representation_model=KeyBERTInspired()\n",
        ")\n",
        "\n",
        "# Fit your BERTopic model and transform documents to get topics and embeddings\n",
        "topics, _ = topic_model.fit_transform(short_docs, embeddings)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get topic summary\n",
        "topic_info_df = topic_model.get_topic_info()"
      ],
      "metadata": {
        "id": "k_O6mOUQ05li"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View topic summary\n",
        "topic_info_df.head(20)"
      ],
      "metadata": {
        "id": "SsOz-VOw05h5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save topic summary to a CSV file\n",
        "\n",
        "topic_info_df = pd.DataFrame(topic_info_df)\n",
        "topic_info_df_file_path = \"summary_ABC.csv\"\n",
        "topic_info_df.to_csv(topic_info_df_file_path, index=False)\n",
        "\n",
        "def upload_to_bucket(bucket_name, source_file_name, destination_blob_name):\n",
        "    \"\"\"Uploads a file to the bucket.\"\"\"\n",
        "\n",
        "    # Initialize a storage client\n",
        "    storage_client = storage.Client()\n",
        "\n",
        "    # Get the bucket\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "\n",
        "    # Create a blob object from the bucket\n",
        "    blob = bucket.blob(destination_blob_name)\n",
        "\n",
        "    # Upload the file\n",
        "    blob.upload_from_filename(source_file_name)\n",
        "\n",
        "    print(f\"File {source_file_name} uploaded to {destination_blob_name}.\")\n",
        "\n",
        "# Define your bucket name\n",
        "bucket_name = \"Your-bucket-name\"\n",
        "\n",
        "# File paths\n",
        "source_file_name = topic_info_df_file_path  # Use the path of the generated embeddings file\n",
        "destination_blob_name = \"path/summary_ABC.csv\"\n",
        "\n",
        "# Upload the file\n",
        "upload_to_bucket(bucket_name, source_file_name, destination_blob_name)"
      ],
      "metadata": {
        "id": "P4XrF8-g1D3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OKjvUevz1D1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pSB3bQf-1Dyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hgGhkYPtSrI"
      },
      "source": [
        "### Part 3. b. BIRCH Topic model (2nd BERTopic model) ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XbGYEhhs8EHX"
      },
      "outputs": [],
      "source": [
        "# Optionally, you can create a new BERTopic model using the BIRCH clusters as topics\n",
        "\n",
        "from sklearn.cluster import Birch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from bertopic import BERTopic\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from bertopic.representation import KeyBERTInspired\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Now, use BIRCH for clustering\n",
        "birch_model = Birch(threshold=0.5, n_clusters=None)  # Adjust threshold and n_clusters as needed\n",
        "birch_model.fit(embeddings)\n",
        "\n",
        "# Predict clusters using BIRCH\n",
        "birch_clusters = birch_model.predict(embeddings)\n",
        "\n",
        "\n",
        "# Create a mapping from BIRCH clusters to documents\n",
        "cluster_to_docs = {}\n",
        "for doc_idx, cluster in enumerate(birch_clusters):\n",
        "    if cluster not in cluster_to_docs:\n",
        "        cluster_to_docs[cluster] = []\n",
        "    cluster_to_docs[cluster].append(doc_idx)\n",
        "\n",
        "# Create a custom topic model with BIRCH clusters\n",
        "custom_topics = []\n",
        "custom_topic_words = []\n",
        "custom_topic_sizes = []\n",
        "\n",
        "for cluster, doc_indices in cluster_to_docs.items():\n",
        "    custom_topics.append(doc_indices)\n",
        "    words = topic_model.get_topic(cluster)  # Get words for the cluster topic\n",
        "    custom_topic_words.append(words)\n",
        "    custom_topic_sizes.append(len(doc_indices))\n",
        "\n",
        "# Create a new BERTopic model with custom topics\n",
        "new_topic_model = BERTopic(\n",
        "    embedding_model=embedding_model,\n",
        "    vectorizer_model=vectorizer_model,\n",
        "    min_topic_size=15,\n",
        "    zeroshot_topic_list=zeroshot_common_topic_list,\n",
        "    zeroshot_min_similarity=.82,\n",
        "    representation_model=KeyBERTInspired()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ov5ZgEGax6BS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming short_docs is your list of document texts\n",
        "df = pd.DataFrame({\n",
        "    'Document': short_docs,\n",
        "    'BIRCH_Cluster': birch_clusters\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWo4wHL6yDD8"
      },
      "outputs": [],
      "source": [
        "# Combine texts by BIRCH cluster\n",
        "cluster_texts = df.groupby('BIRCH_Cluster')['Document'].apply(lambda docs: ' '.join(docs)).reset_index()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xvyzJ1ByT1I"
      },
      "outputs": [],
      "source": [
        "# Fit the new BERTopic model on the combined cluster texts\n",
        "new_topics, _ = new_topic_model.fit_transform(cluster_texts['Document'].tolist())\n",
        "\n",
        "# Get topic summaries for each cluster\n",
        "topic_info = new_topic_model.get_topic_info()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# View topic summary\n",
        "topic_info.head(20)"
      ],
      "metadata": {
        "id": "a6rOBv174B3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save topic summary to a CSV file\n",
        "\n",
        "topic_info = pd.DataFrame(topic_info)\n",
        "topic_info_file_path = \"summary_BIRCH_ABC.csv\"\n",
        "topic_info.to_csv(topic_info_file_path, index=False)\n",
        "\n",
        "def upload_to_bucket(bucket_name, source_file_name, destination_blob_name):\n",
        "    \"\"\"Uploads a file to the bucket.\"\"\"\n",
        "\n",
        "    # Initialize a storage client\n",
        "    storage_client = storage.Client()\n",
        "\n",
        "    # Get the bucket\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "\n",
        "    # Create a blob object from the bucket\n",
        "    blob = bucket.blob(destination_blob_name)\n",
        "\n",
        "    # Upload the file\n",
        "    blob.upload_from_filename(source_file_name)\n",
        "\n",
        "    print(f\"File {source_file_name} uploaded to {destination_blob_name}.\")\n",
        "\n",
        "# Define your bucket name\n",
        "bucket_name = \"Your-bucket-name\"\n",
        "\n",
        "# File paths\n",
        "source_file_name = topic_info_file_path  # Use the path of the generated embeddings file\n",
        "destination_blob_name = \"path/summary_BIRCH_ABC.csv\"\n",
        "\n",
        "# Upload the file\n",
        "upload_to_bucket(bucket_name, source_file_name, destination_blob_name)\n"
      ],
      "metadata": {
        "id": "sGjaNLrs4B0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vjj0e2qB4Bw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0Xj25XTxwFY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VA0jo7sHtIW6"
      },
      "source": [
        "### Part 3. c. BIRCH BERTopic modeling with reduced embedding using PCA  ###"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import Birch\n",
        "import numpy as np\n",
        "\n",
        "# Reduce dimensions of embeddings\n",
        "pca = PCA(n_components=200)  # Adjust the number of components as needed (we tested 180-220)\n",
        "reduced_embeddings = pca.fit_transform(embeddings)\n",
        "\n",
        "# Initialize BIRCH with partial fitting\n",
        "birch_model = Birch(threshold=0.5, n_clusters=None)\n",
        "\n",
        "# Fit BIRCH model incrementally\n",
        "for i in range(0, len(reduced_embeddings), 1000):  # Adjust batch size as needed\n",
        "    batch = reduced_embeddings[i:i + 1000]\n",
        "    birch_model.partial_fit(batch)\n",
        "\n",
        "# Predict clusters using BIRCH\n",
        "birch_clusters = birch_model.predict(reduced_embeddings)\n",
        "\n",
        "# Create a mapping from BIRCH clusters to documents\n",
        "cluster_to_docs = {}\n",
        "for doc_idx, cluster in enumerate(birch_clusters):\n",
        "    if cluster not in cluster_to_docs:\n",
        "        cluster_to_docs[cluster] = []\n",
        "    cluster_to_docs[cluster].append(doc_idx)\n",
        "\n",
        "# Combine texts by BIRCH cluster\n",
        "df = pd.DataFrame({\n",
        "    'Document': short_docs,\n",
        "    'BIRCH_Cluster': birch_clusters\n",
        "})\n",
        "cluster_texts = df.groupby('BIRCH_Cluster')['Document'].apply(lambda docs: ' '.join(docs)).reset_index()\n",
        "\n",
        "# Create a new BERTopic model with custom topics\n",
        "new_topic_model = BERTopic(\n",
        "    embedding_model=embedding_model,\n",
        "    vectorizer_model=vectorizer_model,\n",
        "    min_topic_size=15,\n",
        "    zeroshot_topic_list=zeroshot_common_topic_list,\n",
        "    zeroshot_min_similarity=.82,\n",
        "    representation_model=KeyBERTInspired()\n",
        ")\n",
        "\n",
        "# Fit the new BERTopic model on the combined cluster texts\n",
        "new_topics, _ = new_topic_model.fit_transform(cluster_texts['Document'].tolist())\n",
        "\n",
        "# Get topic summaries for each cluster\n",
        "topic_info = new_topic_model.get_topic_info()\n",
        "\n"
      ],
      "metadata": {
        "id": "drhoqXc0l44T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View topic summary (BIRCH BERTopic with PCA)\n",
        "topic_info.head(30)"
      ],
      "metadata": {
        "id": "UJ9VIEudl41K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate cumulative explained variance\n",
        "explained_variance_ratio = pca.explained_variance_ratio_\n",
        "cumulative_explained_variance = np.cumsum(explained_variance_ratio)\n",
        "\n",
        "# Find the number of components that explain at least 90% variance\n",
        "n_components_90 = np.where(cumulative_explained_variance >= 0.90)[0][0] + 1\n",
        "\n",
        "# Print the number of components\n",
        "print(f\"Number of components explaining 90% variance: {n_components_90}\")\n"
      ],
      "metadata": {
        "id": "2iOlhgabmua2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save topic summary to a CSV file\n",
        "\n",
        "topic_info = pd.DataFrame(topic_info)\n",
        "topic_info_file_path = \"summary_BIRCH_PCA_ABC.csv\"\n",
        "topic_info.to_csv(topic_info_file_path, index=False)\n",
        "\n",
        "def upload_to_bucket(bucket_name, source_file_name, destination_blob_name):\n",
        "    \"\"\"Uploads a file to the bucket.\"\"\"\n",
        "\n",
        "    # Initialize a storage client\n",
        "    storage_client = storage.Client()\n",
        "\n",
        "    # Get the bucket\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "\n",
        "    # Create a blob object from the bucket\n",
        "    blob = bucket.blob(destination_blob_name)\n",
        "\n",
        "    # Upload the file\n",
        "    blob.upload_from_filename(source_file_name)\n",
        "\n",
        "    print(f\"File {source_file_name} uploaded to {destination_blob_name}.\")\n",
        "\n",
        "# Define your bucket name\n",
        "bucket_name = \"Your-bucket-name\"\n",
        "\n",
        "# File paths\n",
        "source_file_name = topic_info_file_path  # Use the path of the generated embeddings file\n",
        "destination_blob_name = \"path/summary_BIRCH_PCA_ABC.csv\"\n",
        "\n",
        "# Upload the file\n",
        "upload_to_bucket(bucket_name, source_file_name, destination_blob_name)"
      ],
      "metadata": {
        "id": "TzEJHOF2LG98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XwMx4tnXLG6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x52T3iWCLcs7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "85_-k2o9Lcpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Correlation test"
      ],
      "metadata": {
        "id": "p5-LA3PZw96e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy import stats\n",
        "import numpy as np\n",
        "\n",
        "# Sample data\n",
        "data = {\n",
        "    'ID': list(range(1, 31)),\n",
        "    'search': [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0],\n",
        "    'score': [3.67, 2.67, 3.00, 3.00, 3.33, 2.67, 3.67, 2.33, 2.33, 5.00, 4.33, 3.00, 3.33, 4.00, 3.00, 3.67, 4.33, 3.67, 4.00, 2.67, 3.33, 3.00, 2.67, 3.33, 3.00, 2.33, 2.67, 3.33, 1.67, 2.67]\n",
        "}\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Perform Spearman correlation test\n",
        "correlation, p_value = stats.spearmanr(df['search'], df['score'])\n",
        "\n",
        "# Function to calculate 95% CI using bootstrapping\n",
        "def bootstrap_ci(data1, data2, num_samples=1000, alpha=0.05):\n",
        "    correlations = []\n",
        "    for _ in range(num_samples):\n",
        "        sample1 = np.random.choice(data1, size=len(data1), replace=True)\n",
        "        sample2 = np.random.choice(data2, size=len(data2), replace=True)\n",
        "        corr, _ = stats.spearmanr(sample1, sample2)\n",
        "        correlations.append(corr)\n",
        "    lower_bound = np.percentile(correlations, 100 * alpha / 2)\n",
        "    upper_bound = np.percentile(correlations, 100 * (1 - alpha / 2))\n",
        "    return lower_bound, upper_bound\n",
        "\n",
        "# Calculate 95% CI using bootstrap\n",
        "ci_lower, ci_upper = bootstrap_ci(df['search'], df['score'])\n",
        "\n",
        "# Print results\n",
        "print(f\"Spearman correlation coefficient: {correlation:.4f}\")\n",
        "print(f\"P-value: {p_value:.4f}\")\n",
        "print(f\"95% Confidence Interval: [{ci_lower:.4f}, {ci_upper:.4f}]\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4z6Er69w8wP",
        "outputId": "5c97b314-c64a-418f-d6b8-7dd7803fd9ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spearman correlation coefficient: 0.2849\n",
            "P-value: 0.1270\n",
            "95% Confidence Interval: [-0.3713, 0.3594]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}